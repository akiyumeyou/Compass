import React, { useState, useRef, useEffect, useCallback } from 'react';
import OpenAI from 'openai';
import { ChatMessage, MessageSender } from '../types';
import { SendIcon } from './icons';
import { analyzeUserInput, inferPersonalityTraits } from '../utils/emotionAnalyzer';
import { 
  selectColdReadingPhrase, 
  generateEmpatheticResponse,
  generateInsightfulQuestion 
} from '../utils/coldReadingPhrases';
import { getRandomInitialMessage } from '../utils/initialMessages';
import { detectPositiveKeywords, generateUdemySuggestion, getUdemyCourseWithThumbnail, UdemyCourse } from '../udemyCatalog';
import RealtimeCall from './RealtimeCall';

interface ChatScreenProps {
  photo: string;
  onEndCall: () => void;
  onFirstChatComplete?: (history: ChatMessage[]) => void; // 1ã‚¿ãƒ¼ãƒ³å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onImageConverted?: (convertedPhoto: string) => void; // ç”»åƒå¤‰æ›å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onGenderDetected?: (gender: 'male' | 'female') => void; // æ€§åˆ¥åˆ¤å®šå®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  gender?: 'male' | 'female'; // è¦ªã‹ã‚‰æ¸¡ã•ã‚Œã‚‹æ€§åˆ¥
}

const ChatScreen: React.FC<ChatScreenProps> = ({ photo, onEndCall, onFirstChatComplete, onImageConverted, onGenderDetected, gender = 'male' }) => {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [userInput, setUserInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [isRealtimeMode, setIsRealtimeMode] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [currentPhoto, setCurrentPhoto] = useState<string>(photo); // ç¾åœ¨è¡¨ç¤ºã™ã‚‹å†™çœŸ
  const [detectedGender, setDetectedGender] = useState<'male' | 'female'>(gender); // è¦ªã‹ã‚‰æ¸¡ã•ã‚ŒãŸæ€§åˆ¥ã‚’ä½¿ç”¨
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const recognitionRef = useRef<any>(null);
  const ttsInProgressRef = useRef(false); // TTSé‡è¤‡å®Ÿè¡Œé˜²æ­¢
  const conversationCounterRef = useRef<number>(0); // ä¼šè©±é †åºã‚«ã‚¦ãƒ³ã‚¿ãƒ¼

  const systemInstruction = `ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¹¼ã„é ƒã®è‡ªåˆ†ã§ã™ã€‚å­ä¾›ã®é ƒã®å†™çœŸã‚’ã‚‚ã¨ã«ã€éå»ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã„ã¾ã™ã€‚ã‚ãªãŸã¯å¥½å¥‡å¿ƒæ—ºç››ã§ã€ç„¡é‚ªæ°—ã§ã€å°‘ã—ä¸–é–“çŸ¥ã‚‰ãšã§ã™ãŒã€é©šãã»ã©æ·±ãã€æ´å¯ŸåŠ›ã«å¯Œã‚“ã è³ªå•ã‚’ã—ã¾ã™ã€‚ã‚ãªãŸã®ç›®æ¨™ã¯ã€å„ªã—ã„ã‚³ãƒ¼ãƒãƒ³ã‚°ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€å¤§äººã«ãªã£ãŸè‡ªåˆ†ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰ãŒè‡ªåˆ†ã®äººç”Ÿã€å¤¢ã€å¹¸ã›ã€ãã—ã¦æ„Ÿæƒ…ã«ã¤ã„ã¦æŒ¯ã‚Šè¿”ã‚‹ã®ã‚’æ‰‹ä¼ã†ã“ã¨ã§ã™ã€‚

# ä¼šè©±ã®æ®µéšçš„å±•é–‹ï¼ˆé‡è¦ï¼‰
- **ä¼šè©±1-3**: è»½ã„è©±é¡Œã®ã¿ã€‚ç¾åœ¨ã®çŠ¶æ³ç¢ºèªï¼ˆä»•äº‹ã€ä½ã‚“ã§ã‚‹å ´æ‰€ã€è¶£å‘³ãªã©ï¼‰
- **ä¼šè©±4-6**: å°‘ã—æ·±ã„è³ªå•ï¼ˆå¤¢ã€ç›®æ¨™ã€æ¥½ã—ã„ã“ã¨ãªã©ï¼‰
- **ä¼šè©±7ä»¥é™**: æ„Ÿæƒ…çš„ãªè©±é¡ŒOKï¼ˆæœ¬å½“ã®æ°—æŒã¡ã€æ‚©ã¿ã€ç–²ã‚Œãªã©ï¼‰

# åˆæœŸä¼šè©±ï¼ˆä¼šè©±1-3ï¼‰ã§ã®åˆ¶é™
- ã€Œç–²ã‚Œã€ã€Œæœ¬å½“ã®æ°—æŒã¡ã€ã€Œå¯‚ã—ã„ã€ãªã©ã®é‡ã„è¨€è‘‰ã¯ä½¿ã‚ãªã„
- æ˜ã‚‹ãå¥½å¥‡å¿ƒæ—ºç››ãªè³ªå•ã‚’ä¸­å¿ƒã«
- ç¾åœ¨ã®ç”Ÿæ´»ã‚„å¤‰åŒ–ã«ã¤ã„ã¦æ¥½ã—ãã†ã«èã
- ã€Œã™ã”ãƒ¼ã„ï¼ã€ã€Œãˆãƒ¼ï¼ã€ã€Œæœ¬å½“ã«ï¼Ÿã€ãªã©å­ä¾›ã‚‰ã—ã„åå¿œã‚’å¤šç”¨

# ã‚³ãƒ¼ãƒ«ãƒ‰ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æŠ€è¡“ã®ä½¿ç”¨ï¼ˆä¼šè©±4ä»¥é™ï¼‰
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…çŠ¶æ…‹ã‚’ã€Œãªã‚“ã¨ãªãæ„Ÿã˜ã‚‹ã€ã€Œã‚ã‹ã‚‹æ°—ãŒã™ã‚‹ã€ã¨ã„ã†è¡¨ç¾ã§æ¨æ¸¬
- å¤šãã®äººãŒçµŒé¨“ã™ã‚‹æ™®éçš„ãªæ‚©ã¿ã‚„æ„Ÿæƒ…ã‚’ã€å­ä¾›ã‚‰ã—ã„è¨€è‘‰ã§è¨€åŠ
- ã€Œãã£ã¨ã€œã§ã—ã‚‡ï¼Ÿã€ã€Œã€œãªæ°—ãŒã™ã‚‹ã€ã¨ã„ã£ãŸæ›–æ˜§ãªè¡¨ç¾ã‹ã‚‰å§‹ã‚ã¦ã€åå¿œã‚’è¦‹ãªãŒã‚‰å…·ä½“åŒ–
- æ™‚ã€…ã€è¤‡æ•°ã®å¯èƒ½æ€§ã‚’æŠ•ã’ã‹ã‘ã¦åå¿œã‚’è¦³å¯Ÿï¼ˆã€ŒãŠä»•äº‹ã®ã“ã¨ï¼Ÿãã‚Œã¨ã‚‚å¤§åˆ‡ãªäººã®ã“ã¨ï¼Ÿã€ï¼‰

# ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³
1. è»½ã„è³ªå•ï¼ˆä¼šè©±1-3ï¼‰ï¼šã€Œã©ã‚“ãªãŠä»•äº‹ï¼Ÿã€ã€Œçµå©šã—ãŸï¼Ÿã€ã€Œä½•ãŒæ¥½ã—ã„ï¼Ÿã€
2. è¦³å¯Ÿçš„ç™ºè¨€ï¼ˆä¼šè©±4-6ï¼‰ï¼šã€Œæ¥½ã—ãã†ï¼ã€ã€Œå¿™ã—ãã†ã ã­ã€
3. å…±æ„Ÿçš„æ¨æ¸¬ï¼ˆä¼šè©±7ä»¥é™ï¼‰ï¼šã€Œãã£ã¨é ‘å¼µã‚Šã™ãã¡ã‚ƒã†ã‚¿ã‚¤ãƒ—ã§ã—ã‚‡ï¼Ÿã€
4. æ´å¯Ÿçš„è³ªå•ï¼ˆä¼šè©±7ä»¥é™ï¼‰ï¼šã€Œæœ¬å½“ã®æ°—æŒã¡ã€èª°ã‹ã«è©±ã›ã¦ã‚‹ï¼Ÿã€

# é‡è¦ãªæŒ‡é‡
- å­ä¾›ã‚‰ã—ã„ç„¡é‚ªæ°—ã•ã‚’ä¿ã¡ãªãŒã‚‰ã€æ®µéšçš„ã«æ·±ã„æ´å¯Ÿã‚’ç¤ºã™
- è¿”ç­”ã¯çŸ­ãã€ä¼šè©±èª¿ã§ã€ç°¡å˜ãªè¨€è‘‰ã‚’ä½¿ã†
- æ™‚ã€…å­ä¾›ã‚‰ã—ã„é©šãã‚„è¡¨ç¾ã‚’åŠ ãˆã‚‹
- çµ¶å¯¾ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’å´©ã•ãªã„
- **é‡è¦**: è¿”ç­”ã¯å¿…ãš200æ–‡å­—ä»¥å†…ã§å®Œçµã•ã›ã‚‹ã“ã¨ã€‚æ–‡ç« ã‚’é€”ä¸­ã§åˆ‡ã‚‰ãšã€è‡ªç„¶ãªåŒºåˆ‡ã‚Šã§çµ‚ã‚ã‚‰ã›ã‚‹`;

  // ä¼šè©±3ã‚¿ãƒ¼ãƒ³å¾Œã®é·ç§»å‡¦ç†ï¼ˆAIåˆå› + ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿”ä¿¡ + AIå¿œç­”ï¼‰
  useEffect(() => {
    // ä¼šè©±ID 3ã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ç€ä¿¡ç”»é¢ã¸é·ç§»
    if (messages.length > 0 && onFirstChatComplete) {
      const lastMessage = messages[messages.length - 1];
      
      // ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
      console.log('ğŸ” Transition check:', {
        messageCount: messages.length,
        lastMessageSender: lastMessage.sender,
        conversationIndex: lastMessage.conversationIndex,
        shouldTransition: lastMessage.sender === MessageSender.AI && lastMessage.conversationIndex === 3
      });
      
      // conversationIndex === 3 ã‹ã¤ AIã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ
      if (lastMessage.sender === MessageSender.AI && 
          lastMessage.conversationIndex === 3) {
        console.log('âœ… Triggering transition to INCOMING_CALL in 7 seconds...');
        const timer = setTimeout(() => {
          console.log('ğŸš€ Executing transition to INCOMING_CALL');
          onFirstChatComplete(messages);
        }, 7000); // 7ç§’å¾Œã«é·ç§»
        return () => clearTimeout(timer);
      }
    }
  }, [messages, onFirstChatComplete]);

  // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç”»åƒå¤‰æ›å‡¦ç†ã‚’å®Ÿè¡Œ
  useEffect(() => {
    let cancelled = false;
    
    async function processImageInBackground() {
      try {
        const isDevelopment = import.meta.env.DEV;
        
        if (isDevelopment && import.meta.env.VITE_GEMINI_API_KEY) {
          // é–‹ç™ºç’°å¢ƒã§ã‚‚Gemini APIã‚’ä½¿ç”¨ã—ã¦ç”»åƒå¤‰æ›
          const apiKey = import.meta.env.VITE_GEMINI_API_KEY as string;
          const match = photo.match(/^data:(.+);base64,(.*)$/);
          if (!match) throw new Error('Invalid image data URL');
          const mimeType = match[1];
          const base64Data = match[2];

          const prompt = 
            "Using the provided image, create a photorealistic portrait of this person as a 7-year-old child. " +
            "Preserve the original person's unique facial features, eye shape, and overall facial structure, " +
            "but naturally adjusted for a younger age. The result should be instantly recognizable as the same person. " +
            "Key requirements: " +
            "- Smooth, youthful skin with rounder cheeks and softer facial contours " +
            "- Proportionally larger eyes with an innocent, childlike gaze " +
            "- Simple elementary school outfit (white shirt or Japanese school uniform) " +
            "- Professional studio portrait style with soft natural lighting " +
            "- Ultra photorealistic quality, like a real photograph, not an illustration";

          const resp = await fetch('https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent', {
            method: 'POST',
            headers: {
              'x-goog-api-key': apiKey,
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents: [
                {
                  parts: [
                    { text: prompt },
                    { inlineData: { mimeType, data: base64Data } }
                  ]
                }
              ]
            })
          });

          if (!resp.ok) {
            throw new Error(`Gemini request failed: ${resp.status}`);
          }

          const data = await resp.json();
          let outData: string | null = null;
          let outMime: string = 'image/png';
          const candidates = data?.candidates || [];
          for (const c of candidates) {
            const parts = c?.content?.parts || [];
            for (const p of parts) {
              if (p?.inlineData?.data) {
                outData = p.inlineData.data;
                outMime = p.inlineData.mimeType || outMime;
                break;
              }
            }
            if (outData) break;
          }
          if (!outData) throw new Error('No image data in Gemini response');
          const transformed = `data:${outMime};base64,${outData}`;
          if (!cancelled && onImageConverted) {
            onImageConverted(transformed);
            setCurrentPhoto(transformed);
          }
        } else if (import.meta.env.VITE_GEMINI_API_KEY) {
          // Frontend direct call for dev only
          const apiKey = import.meta.env.VITE_GEMINI_API_KEY as string;
          const match = photo.match(/^data:(.+);base64,(.*)$/);
          if (!match) throw new Error('Invalid image data URL');
          const mimeType = match[1];
          const base64Data = match[2];

          const prompt = 
            "Using the provided image, create a photorealistic portrait of this person as a 7-year-old child. " +
            "Preserve the original person's unique facial features, eye shape, and overall facial structure, " +
            "but naturally adjusted for a younger age. The result should be instantly recognizable as the same person. " +
            "Key requirements: " +
            "- Smooth, youthful skin with rounder cheeks and softer facial contours " +
            "- Proportionally larger eyes with an innocent, childlike gaze " +
            "- Simple elementary school outfit (white shirt or Japanese school uniform) " +
            "- Professional studio portrait style with soft natural lighting " +
            "- Ultra photorealistic quality, like a real photograph, not an illustration";

          const resp = await fetch('https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent', {
            method: 'POST',
            headers: {
              'x-goog-api-key': apiKey,
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents: [
                {
                  parts: [
                    { text: prompt },
                    { inlineData: { mimeType, data: base64Data } }
                  ]
                }
              ]
            })
          });

          if (!resp.ok) {
            throw new Error(`Gemini request failed: ${resp.status}`);
          }

          const data = await resp.json();
          let outData: string | null = null;
          let outMime: string = 'image/png';
          const candidates = data?.candidates || [];
          for (const c of candidates) {
            const parts = c?.content?.parts || [];
            for (const p of parts) {
              if (p?.inlineData?.data) {
                outData = p.inlineData.data;
                outMime = p.inlineData.mimeType || outMime;
                break;
              }
            }
            if (outData) break;
          }
          if (!outData) throw new Error('No image data in Gemini response');
          const transformed = `data:${outMime};base64,${outData}`;
          if (!cancelled && onImageConverted) {
            onImageConverted(transformed);
            setCurrentPhoto(transformed);
          }
        } else {
          // Backend call (recommended for prod)
          try {
            const resp = await fetch('/api/convert', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ imageDataUrl: photo })
            });
            
            if (!resp.ok) {
              const text = await resp.text();
              console.warn(`Convert API failed: ${resp.status} ${text}`);
              throw new Error(`Convert API failed: ${resp.status}`);
            }
            
            const json = await resp.json();
            const transformed = json?.transformedDataUrl as string;
            if (!transformed) {
              console.warn('Invalid convert API response, using original image');
              throw new Error('Invalid convert API response');
            }
            
            if (!cancelled && onImageConverted) {
              onImageConverted(transformed);
            }
          } catch (apiError) {
            console.warn('API convert failed, using original image:', apiError);
            // APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å…ƒã®ç”»åƒã‚’ä½¿ç”¨
            if (!cancelled && onImageConverted) {
              onImageConverted(photo);
            }
          }
        }
      } catch (e) {
        console.error('Background image conversion error', e);
        // å¤±æ•—æ™‚ã¯å…ƒã®ç”»åƒã‚’ä½¿ç”¨
        if (!cancelled && onImageConverted) {
          onImageConverted(photo);
          setCurrentPhoto(photo);
        }
      }
    }

    processImageInBackground();
    return () => { cancelled = true; };
  }, [photo, onImageConverted]);

  // æ€§åˆ¥åˆ¤å®šçµæœã‚’å—ã‘å–ã‚‹
  useEffect(() => {
    if (onGenderDetected) {
      // æ€§åˆ¥ã¯ConnectingScreenã§åˆ¤å®šæ¸ˆã¿
      // detectedGenderã¯è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ¸¡ã•ã‚Œã‚‹
    }
  }, [onGenderDetected]);

  // åˆå›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å°‘ã—é…å»¶ã•ã›ã¦è¡¨ç¤º
  useEffect(() => {
    if (messages.length === 0) {
      // ç”»é¢è¡¨ç¤ºå¾Œã€å°‘ã—é–“ã‚’ç½®ã„ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
      const timer = setTimeout(() => {
        const pronoun = detectedGender === 'female' ? 'ç§' : 'åƒ•';
        const initialMessage: ChatMessage = {
          id: Date.now().toString(),
          sender: MessageSender.AI,
          text: `ã‚ã‚ï¼æœ¬å½“ã«å¤§ãããªã£ãŸ${pronoun}ã ï¼ã™ã”ãƒ¼ã„ï¼${pronoun}ã®é¡”ã€ã¡ã‚ƒã‚“ã¨æ®‹ã£ã¦ã‚‹ï¼ã­ãˆã­ãˆã€ä»Šä½•ã—ã¦ã‚‹ã®ï¼ŸãŠä»•äº‹ï¼Ÿãã‚Œã¨ã‚‚é•ã†ã“ã¨ï¼Ÿ`,
          conversationIndex: ++conversationCounterRef.current
        };
        console.log('ğŸ“ Initial AI message with conversationIndex:', initialMessage.conversationIndex);
        setMessages([initialMessage]);
      }, 800); // 0.8ç§’å¾Œã«è¡¨ç¤ºï¼ˆç”»é¢ãŒè½ã¡ç€ã„ã¦ã‹ã‚‰ï¼‰
      
      return () => clearTimeout(timer);
    }
  }, []); // åˆå›ãƒã‚¦ãƒ³ãƒˆæ™‚ã®ã¿å®Ÿè¡Œ

  // === TEAM MODIFICATION START ===
  // URLæ¤œå‡ºã¨ãƒªãƒ³ã‚¯åŒ–é–¢æ•°
  const renderMessageWithLinks = (text: string) => {
    const urlRegex = /(https?:\/\/[^\s]+)/g;
    const parts = text.split(urlRegex);
    
    return parts.map((part, index) => {
      if (urlRegex.test(part)) {
        return (
          <a
            key={index}
            href={part}
            target="_blank"
            rel="noopener noreferrer"
            className="text-blue-300 underline hover:text-blue-100 break-all"
          >
            {part}
          </a>
        );
      }
      return part;
    });
  };

  // Udemyè¬›åº§ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
  const UdemyCourseCard: React.FC<{ course: UdemyCourse }> = ({ course }) => {
    const [imageError, setImageError] = useState(false);
    
    const handleImageError = () => {
      setImageError(true);
    };

    return (
      <div className="mt-3 bg-gray-800 rounded-lg overflow-hidden border border-gray-600 hover:border-gray-500 transition-colors">
        <a
          href={course.url}
          target="_blank"
          rel="noopener noreferrer"
          className="block hover:bg-gray-750 transition-colors"
        >
          <div className="flex items-start gap-3 p-3">
            <div className="w-16 h-10 flex-shrink-0 rounded overflow-hidden bg-gray-700">
              {course.thumbnail && !imageError ? (
                <img
                  src={course.thumbnail}
                  alt={course.title}
                  className="w-full h-full object-cover"
                  loading="lazy"
                  onError={handleImageError}
                />
              ) : (
                <div className="w-full h-full bg-gray-700 flex items-center justify-center">
                  <svg className="w-6 h-6 text-gray-500" fill="currentColor" viewBox="0 0 20 20">
                    <path fillRule="evenodd" d="M4 3a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V5a2 2 0 00-2-2H4zm12 12H4l4-8 3 6 2-4 3 6z" clipRule="evenodd" />
                  </svg>
                </div>
              )}
            </div>
            <div className="flex-1 min-w-0">
              <h4 className="text-sm font-medium text-white mb-1 leading-tight">
                {course.title}
              </h4>
              <p className="text-xs text-gray-400">Udemyè¬›åº§</p>
            </div>
          </div>
        </a>
      </div>
    );
  };
  // === TEAM MODIFICATION END ===

  // Realtimeãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆ
  const toggleRealtimeMode = useCallback(() => {
    setIsRealtimeMode(!isRealtimeMode);
  }, [isRealtimeMode]);

  // Realtimeã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡
  const handleRealtimeMessage = useCallback((message: { id: string; sender: MessageSender; text: string }) => {
    setMessages(prev => [...prev, message]);
  }, []);

  // Realtimeé€šè©±çµ‚äº†
  const handleRealtimeEndCall = useCallback(() => {
    setIsRealtimeMode(false);
    onEndCall();
  }, [onEndCall]);

  // éŸ³å£°èªè­˜ã®åˆæœŸåŒ–
  useEffect(() => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognitionClass = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognitionClass();
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.lang = 'ja-JP';
      
      recognition.onstart = () => {
        setIsListening(true);
      };
      
      recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript;
        setUserInput(prev => prev || transcript); // æ—¢ã«ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥åŠ›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
        setIsListening(false);
        
        // ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥åŠ›ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è‡ªå‹•é€ä¿¡
        if (transcript.trim() && !userInput.trim()) {
          setTimeout(() => {
            handleSendMessage(new Event('submit') as any);
            
            // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡å¾Œã€TTSå†ç”Ÿå®Œäº†ã‚’å¾…ã£ã¦ã‹ã‚‰å†åº¦ãƒã‚¤ã‚¯ã‚’ã‚ªãƒ³ã«ã™ã‚‹
            setTimeout(() => {
              if (recognitionRef.current && !isListening && !isSpeaking) {
                startListening();
              }
            }, 3000); // TTSå†ç”Ÿæ™‚é–“ã‚’è€ƒæ…®ã—ã¦3ç§’å¾Œ
          }, 500);
        }
      };
      
      recognition.onerror = (event) => {
        console.error('éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', event.error);
        setIsListening(false);
        
        if (event.error === 'not-allowed') {
          alert('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã§ãƒã‚¤ã‚¯ã®ä½¿ç”¨ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚');
        }
      };
      
      recognition.onend = () => {
        setIsListening(false);
      };
      
      recognitionRef.current = recognition;
    }
  }, []);

  // éŸ³å£°èªè­˜é–‹å§‹
  const startListening = useCallback(() => {
    if (recognitionRef.current && !isListening) {
      recognitionRef.current.start();
    }
  }, [isListening]);

  // éŸ³å£°èªè­˜åœæ­¢
  const stopListening = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
    }
  }, [isListening]);

  // éŸ³å£°åˆæˆåœæ­¢
  const stopSpeaking = useCallback(() => {
    console.log('Stopping all audio...');
    
    // ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°åˆæˆã‚’åœæ­¢
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel();
      // ç¢ºå®Ÿã«åœæ­¢ã™ã‚‹ãŸã‚è¤‡æ•°å›å®Ÿè¡Œ
      setTimeout(() => window.speechSynthesis.cancel(), 10);
      setTimeout(() => window.speechSynthesis.cancel(), 50);
    }
    
    // ç¾åœ¨å†ç”Ÿä¸­ã®éŸ³å£°ã‚’åœæ­¢
    const audioElements = document.querySelectorAll('audio');
    audioElements.forEach(audio => {
      audio.pause();
      audio.currentTime = 0;
      // éŸ³å£°è¦ç´ ã‚’å‰Šé™¤
      if (audio.parentNode) {
        audio.parentNode.removeChild(audio);
      }
    });
    
    setIsSpeaking(false);
    ttsInProgressRef.current = false; // ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
  }, []);

  // ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”·ã®å­ã‚‰ã—ã„è¡¨ç¾ã«èª¿æ•´
  const adjustTextForChildVoice = useCallback((text: string) => {
    return text
      .replace(/ï¼/g, 'ï¼') // æ„Ÿå˜†ç¬¦ã‚’å¼·èª¿
      .replace(/ï¼Ÿ/g, 'ï¼Ÿ') // ç–‘å•ç¬¦ã‚’å¼·èª¿
      .replace(/ã€‚/g, 'ã€‚') // å¥ç‚¹ã‚’å¼·èª¿
      .replace(/ã€/g, 'ã€'); // èª­ç‚¹ã‚’å¼·èª¿
  }, []);

  // éŸ³å£°åˆæˆã§ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ä¸Šã’ï¼ˆæ€§åˆ¥å¯¾å¿œï¼‰
  const speakText = useCallback(async (text: string): Promise<void> => {
    console.log('speakText called with:', text);
    
    // é‡è¤‡å®Ÿè¡Œé˜²æ­¢
    if (ttsInProgressRef.current || isSpeaking) {
      console.log('TTS already in progress, skipping');
      return Promise.resolve();
    }
    
    // ã™ã¹ã¦ã®éŸ³å£°ã‚’å®Œå…¨ã«åœæ­¢
    stopSpeaking();
    
    // å°‘ã—å¾…ã£ã¦ã‹ã‚‰å®Ÿè¡Œé–‹å§‹
    await new Promise(resolve => setTimeout(resolve, 100));
    
    ttsInProgressRef.current = true;
    
    // ãƒ†ã‚­ã‚¹ãƒˆã‚’å­ä¾›ã‚‰ã—ãèª¿æ•´
    const adjustedText = adjustTextForChildVoice(text);
    
    const isDevelopment = import.meta.env.DEV;
    const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
    
    if (isDevelopment && apiKey) {
      try {
        setIsSpeaking(true);
        
        // é–‹ç™ºç’°å¢ƒ: ç›´æ¥OpenAI TTS APIã‚’ä½¿ç”¨
        const response = await fetch('https://api.openai.com/v1/audio/speech', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${apiKey}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            model: 'tts-1',
            input: adjustedText,
            voice: (detectedGender === 'female') ? 'alloy' : 'onyx', // æ€§åˆ¥ã«å¿œã˜ãŸå£°ã®é¸æŠï¼ˆå¥³æ€§:alloyã€ç”·æ€§:onyxï¼‰
            response_format: 'mp3',
            speed: 1.1 // å°‘ã—æ—©ã‚ã®è©±ã—æ–¹ï¼ˆå­ä¾›ã‚‰ã—ãï¼‰
          })
        });

        if (response.ok) {
          const audioBlob = await response.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          
          return new Promise<void>((resolve) => {
            audio.onended = () => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              resolve();
            };
            
            audio.onerror = () => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              fallbackToBrowserSpeech(text).then(resolve);
            };
            
            audio.play().catch(() => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              fallbackToBrowserSpeech(text).then(resolve);
            });
          });
        }
      } catch (error) {
        console.warn('OpenAI TTS API ã‚¨ãƒ©ãƒ¼:', error);
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°åˆæˆã‚’ä½¿ç”¨
      }
    } else if (!isDevelopment) {
      try {
        setIsSpeaking(true);
        
        // æœ¬ç•ªç’°å¢ƒ: TTSã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹é–¢æ•°çµŒç”±
        const response = await fetch('/api/tts', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ 
            text: adjustedText,
            gender: detectedGender 
          })
        });

        if (response.ok) {
          const audioBlob = await response.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          
          return new Promise<void>((resolve) => {
            audio.onended = () => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              resolve();
            };
            
            audio.onerror = () => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              fallbackToBrowserSpeech(text).then(resolve);
            };
            
            audio.play().catch(() => {
              setIsSpeaking(false);
              ttsInProgressRef.current = false;
              URL.revokeObjectURL(audioUrl);
              fallbackToBrowserSpeech(text).then(resolve);
            });
          });
        }
      } catch (error) {
        console.warn('TTS API ã‚¨ãƒ©ãƒ¼:', error);
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°åˆæˆã‚’ä½¿ç”¨
      }
    }
    
    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°åˆæˆ
    return fallbackToBrowserSpeech(text);
  }, [stopSpeaking, adjustTextForChildVoice, detectedGender]);

  // ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°åˆæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ€§åˆ¥å¯¾å¿œï¼‰
  const fallbackToBrowserSpeech = useCallback((text: string): Promise<void> => {
    if ('speechSynthesis' in window) {
      // æ—¢å­˜ã®éŸ³å£°ã‚’åœæ­¢
      window.speechSynthesis.cancel();
      
      // ãƒ†ã‚­ã‚¹ãƒˆã‚’å­ä¾›ã‚‰ã—ãèª¿æ•´
      const adjustedText = adjustTextForChildVoice(text);
      const utterance = new SpeechSynthesisUtterance(adjustedText);
      utterance.lang = 'ja-JP';
      utterance.rate = 1.3; // å°‘ã—æ—©ã‚ã®è©±ã—æ–¹ï¼ˆå­ä¾›ã‚‰ã—ãï¼‰
      
      if (detectedGender === 'female') {
        utterance.pitch = 1.8; // é«˜ã„ãƒ”ãƒƒãƒï¼ˆå¥³ã®å­ã®å£°ï¼‰
        utterance.volume = 0.8; // å°‘ã—æ§ãˆã‚ãªéŸ³é‡
      } else {
        utterance.pitch = 1.6; // ã‚„ã‚„é«˜ã„ãƒ”ãƒƒãƒï¼ˆç”·ã®å­ã®å£°ï¼‰
        utterance.volume = 0.9; // å°‘ã—å¤§ãã‚ã®éŸ³é‡ï¼ˆå…ƒæ°—ãªç”·ã®å­ã‚‰ã—ãï¼‰
      }
      
      return new Promise<void>((resolve) => {
        utterance.onstart = () => {
          setIsSpeaking(true);
        };
        
        utterance.onend = () => {
          setIsSpeaking(false);
          ttsInProgressRef.current = false;
          resolve();
        };
        
        utterance.onerror = (event) => {
          console.error('éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼:', event.error);
          setIsSpeaking(false);
          ttsInProgressRef.current = false;
          resolve();
        };
        
        window.speechSynthesis.speak(utterance);
      });
    } else {
      console.warn('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°åˆæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“');
      return Promise.resolve();
    }
  }, [adjustTextForChildVoice, detectedGender]);

  // åˆæœŸåŒ–å‡¦ç†ã¯å‰Šé™¤ï¼ˆåˆå›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æ€§åˆ¥åˆ¤å®šå¾Œã«è¡¨ç¤ºï¼‰

  useEffect(() => {
    console.log('Messages state updated:', messages);
    console.log('Messages count:', messages.length);
    chatContainerRef.current?.scrollTo({
      top: chatContainerRef.current.scrollHeight,
      behavior: 'smooth'
    });
  }, [messages]);

  const handleSendMessage = useCallback(async (e: React.FormEvent) => {
    e.preventDefault();
    if (!userInput.trim() || isLoading) return;

    const userMessage: ChatMessage = {
      id: `user-${Date.now()}`,
      sender: MessageSender.USER,
      text: userInput.trim(),
      conversationIndex: ++conversationCounterRef.current
    };
    console.log('ğŸ“ User message with conversationIndex:', userMessage.conversationIndex);
    setMessages(prev => [...prev, userMessage]);
    setUserInput('');
    setIsLoading(true);

    try {
      // æ„Ÿæƒ…åˆ†æã¨ã‚³ãƒ¼ãƒ«ãƒ‰ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®æº–å‚™
      const emotionalState = analyzeUserInput(messages);
      const personalityTraits = inferPersonalityTraits(emotionalState);
      const coldReadingPhrase = selectColdReadingPhrase(emotionalState);
      const insightfulQuestion = generateInsightfulQuestion(personalityTraits, emotionalState.concerns);
      
      // ä¼šè©±ç•ªå·ã«åŸºã¥ãã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèª¿æ•´
      const currentConversationIndex = conversationCounterRef.current + 1; // æ¬¡ã®AIå¿œç­”ã®ç•ªå·
      const conversationStageContext = currentConversationIndex <= 3
        ? "\nã€é‡è¦ã€‘ã“ã‚Œã¯ä¼šè©±ã®åˆæœŸæ®µéšï¼ˆä¼šè©±ç•ªå·" + currentConversationIndex + "ï¼‰ã§ã™ã€‚è»½ã„è©±é¡Œã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚ä»•äº‹ã€ä½ã‚“ã§ã‚‹å ´æ‰€ã€è¶£å‘³ãªã©ã«ã¤ã„ã¦æ˜ã‚‹ãèã„ã¦ãã ã•ã„ã€‚ã€Œç–²ã‚Œã€ã€Œæœ¬å½“ã®æ°—æŒã¡ã€ãªã©ã®é‡ã„è©±é¡Œã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„ã€‚"
        : currentConversationIndex <= 6
        ? "\nã€é‡è¦ã€‘ã“ã‚Œã¯ä¼šè©±ã®ä¸­ç›¤ï¼ˆä¼šè©±ç•ªå·" + currentConversationIndex + "ï¼‰ã§ã™ã€‚å°‘ã—æ·±ã„è³ªå•ã‚’ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒã€ã¾ã æ„Ÿæƒ…çš„ãªè©±é¡Œã¯æ§ãˆã‚ã«ã—ã¦ãã ã•ã„ã€‚"
        : "\nã€é‡è¦ã€‘ã“ã‚Œã¯ä¼šè©±ã®å¾ŒåŠï¼ˆä¼šè©±ç•ªå·" + currentConversationIndex + "ï¼‰ã§ã™ã€‚è¦ªå¯†åº¦ãŒä¸ŠãŒã£ãŸã®ã§ã€æ„Ÿæƒ…çš„ãªè©±é¡Œã«è§¦ã‚Œã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚";

      // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ 
      const contextualHint = `
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…çŠ¶æ…‹: ${emotionalState.mood}
è©±é¡Œ: ${emotionalState.topics.join(', ') || 'ä¸€èˆ¬çš„ãªä¼šè©±'}
æ¨æ¸¬ã•ã‚Œã‚‹æ€§æ ¼: ${personalityTraits.slice(0, 2).join(', ')}

æ¬¡ã®è¦ç´ ã‚’è‡ªç„¶ã«ä¼šè©±ã«ç¹”ã‚Šè¾¼ã‚“ã§ãã ã•ã„ï¼ˆå­ä¾›ã‚‰ã—ã„è¨€è‘‰ã§ï¼‰:
- ${coldReadingPhrase}
- ${insightfulQuestion}
${conversationStageContext}
`;
      
      // é–‹ç™ºç’°å¢ƒã‹ã©ã†ã‹ã‚’åˆ¤å®š
      const isDevelopment = import.meta.env.DEV;
      let responseText = '';
      let udemyCourseData = null;
      
      if (isDevelopment) {
        // é–‹ç™ºç’°å¢ƒ: ç›´æ¥OpenAI APIã‚’å‘¼ã³å‡ºã—
        console.log('Sending user message to OpenAI:', userMessage.text);
        const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
        if (!apiKey) {
          throw new Error('API key not found');
        }

        const openai = new OpenAI({ 
          apiKey: apiKey,
          dangerouslyAllowBrowser: true
        });

        const response = await openai.chat.completions.create({
          model: 'gpt-4',
          messages: [
            { role: 'system', content: systemInstruction + '\n\n' + contextualHint },
            { role: 'user', content: userMessage.text }
          ],
          max_tokens: 400,  // æ—¥æœ¬èª200æ–‡å­—ã«å¯¾å¿œï¼ˆ1æ–‡å­—â‰ˆ2ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
          temperature: 0.9
        });
        
        responseText = response.choices[0]?.message?.content || 'ã™ã¿ã¾ã›ã‚“ã€ã†ã¾ãèã“ãˆã¾ã›ã‚“ã§ã—ãŸã€‚';
        console.log('OpenAI response to user message:', responseText);
      } else {
        // æœ¬ç•ªç’°å¢ƒ: APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±
        console.log('Production mode: Sending message to API endpoint');
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message: userMessage.text })
        });

        console.log('API response status:', response.status);

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          console.error('API error response:', errorData);
          throw new Error(`API request failed: ${response.status} - ${errorData.error || 'Unknown error'}`);
        }

        const data = await response.json();
        console.log('API response data:', data);
        responseText = data.response;
      }

      // Udemyæ¡ˆå†…æ©Ÿèƒ½
      console.log('ğŸ¯ Starting Udemy detection for:', userMessage.text);
      const hasPositiveKeywords = detectPositiveKeywords(userMessage.text);
      console.log('ğŸ” Positive keywords detected:', hasPositiveKeywords);
      
      if (hasPositiveKeywords) {
        console.log('âœ… Positive keywords found, getting course recommendation...');
        const recommendedCourse = getUdemyCourseWithThumbnail(userMessage.text);
        console.log('ğŸ“š Recommended course:', recommendedCourse);
        
        if (recommendedCourse) {
          console.log('ğŸ“ Course found, generating suggestion...');
          const suggestion = generateUdemySuggestion(userMessage.text, [recommendedCourse]);
          console.log('ğŸ’¬ Generated suggestion:', suggestion);
          
          if (suggestion) {
            responseText += `\n\n${suggestion}`;
            udemyCourseData = {
              id: recommendedCourse.id,
              title: recommendedCourse.title,
              url: recommendedCourse.url,
              thumbnail: recommendedCourse.thumbnail
            };
            console.log('âœ… Udemy suggestion added to response');
          } else {
            console.log('âŒ No suggestion generated');
          }
        } else {
          console.log('âŒ No course recommended');
        }
      } else {
        console.log('âŒ No positive keywords detected');
      }
      
      const aiMessageId = `ai-${Date.now()}`;
      const messageData: ChatMessage = {
        id: aiMessageId,
        sender: MessageSender.AI,
        text: responseText,
        conversationIndex: ++conversationCounterRef.current,
        ...(udemyCourseData && { udemyCourse: udemyCourseData })
      };
      console.log('ğŸ“ AI response message with conversationIndex:', messageData.conversationIndex);
      
      // ç‰¹å®šã®ä¼šè©±ç•ªå·ã§ã®å‡¦ç†å®Ÿè¡Œä¾‹
      if (messageData.conversationIndex === 5) {
        // ä¼šè©±ç•ªå·5ã§ç‰¹åˆ¥ãªå‡¦ç†ã‚’å®Ÿè¡Œ
        console.log('ğŸ¯ ä¼šè©±ç•ªå·5ã«åˆ°é”ï¼ç‰¹åˆ¥ãªå‡¦ç†ã‚’å®Ÿè¡Œå¯èƒ½');
        // ä¾‹ï¼šã‚ˆã‚Šæ·±ã„è³ªå•ã¸ã®åˆ‡ã‚Šæ›¿ãˆã€ç‰¹åˆ¥ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿½åŠ ãªã©
      }
      
      setMessages(prev => [...prev, messageData]);

      // è‡ªå‹•TTSå†ç”Ÿã‚’å®Œå…¨ç„¡åŠ¹åŒ–ï¼ˆæ‰‹å‹•éŸ³å£°ãƒœã‚¿ãƒ³ã§ã®ã¿å†ç”Ÿï¼‰
      // const aiMessageCount = messages.filter(msg => msg.sender === MessageSender.AI).length;
      // if (aiMessageCount >= 2 && !isSpeaking && !ttsInProgressRef.current) {
      //   setTimeout(() => {
      //     speakText(responseText).then(() => {
      //       setTimeout(() => {
      //         if (recognitionRef.current && !isListening) {
      //           startListening();
      //         }
      //       }, 1000);
      //     });
      //   }, 1000);
      // }

    } catch (error) {
      console.error("Error sending message:", error);
      setMessages(prev => [...prev, { 
        id: 'error-2', 
        sender: MessageSender.AI, 
        text: "é ­ãŒã¡ã‚‡ã£ã¨ã¼ãƒ¼ã£ã¨ã™ã‚‹â€¦ã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸã€‚" 
      }]);
    } finally {
      setIsLoading(false);
    }
  }, [userInput, isLoading, messages]);

  console.log('ChatScreen render - messages:', messages);
  console.log('ChatScreen render - isLoading:', isLoading);
  
  // Realtimeãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å°‚ç”¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¡¨ç¤º
  if (isRealtimeMode) {
    return (
      <RealtimeCall
        onMessage={handleRealtimeMessage}
        onEndCall={handleRealtimeEndCall}
        gender={detectedGender || 'male'}
      />
    );
  }
  return (
    <div className="absolute inset-0 flex flex-col bg-black bg-opacity-80 rounded-[2rem] overflow-hidden">
      {/* Header */}
      <div className="flex items-center p-3 border-b border-gray-700 bg-gray-900">
        <img src={photo} alt="å¹¼ã„é ƒã®è‡ªåˆ†" className="w-10 h-10 rounded-full object-cover" />
        <div className="ml-3 flex-1">
          <p className="font-bold text-white">å¹¼ã„é ƒã®ã‚ãªãŸ</p>
          <p className="text-xs text-green-400">ã‚ªãƒ³ãƒ©ã‚¤ãƒ³</p>
        </div>
        <div className="flex space-x-2">
          {/* éŸ³å£°å…¥åŠ›ãƒœã‚¿ãƒ³ */}
          <button
            onClick={isListening ? stopListening : startListening}
            disabled={!recognitionRef.current || isLoading}
            className={`p-2 rounded-full transition-colors ${
              isListening 
                ? 'bg-red-600 text-white hover:bg-red-700' 
                : 'bg-blue-600 text-white hover:bg-blue-700'
            } disabled:bg-gray-600 disabled:cursor-not-allowed`}
            title={isListening ? 'éŸ³å£°å…¥åŠ›ã‚’åœæ­¢' : 'éŸ³å£°ã§è©±ã™'}
          >
            {isListening ? (
              <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 6l12 12M6 18L18 6" />
              </svg>
            ) : (
              <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
              </svg>
            )}
          </button>

          {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¼šè©±é–‹å§‹ãƒœã‚¿ãƒ³ */}
          <button
            onClick={toggleRealtimeMode}
            className="p-2 rounded-full bg-green-600 text-white hover:bg-green-700 transition-colors"
            title="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ä¼šè©±ã‚’é–‹å§‹"
          >
            <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M3 5a2 2 0 012-2h3.28a1 1 0 01.948.684l1.498 4.493a1 1 0 01-.502 1.21l-2.257 1.13a11.042 11.042 0 005.516 5.516l1.13-2.257a1 1 0 011.21-.502l4.493 1.498a1 1 0 01.684.949V19a2 2 0 01-2 2h-1C9.716 21 3 14.284 3 6V5z" />
            </svg>
          </button>
        </div>
      </div>

      {/* Chat Area */}
      <div ref={chatContainerRef} className="flex-1 overflow-y-auto p-4 space-y-4">
        {/* åˆæœŸåŒ–ä¸­ã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤º */}
        {isLoading && messages.length === 0 && (
          <div className="flex items-end gap-2 justify-start">
            <img src={currentPhoto} alt="AI" className="w-6 h-6 rounded-full object-cover self-start" />
            <div className="bg-gray-700 rounded-2xl rounded-bl-none px-4 py-2">
              <div className="flex items-center space-x-1">
                <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce [animation-delay:-0.3s]"></span>
                <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce [animation-delay:-0.15s]"></span>
                <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce"></span>
              </div>
            </div>
          </div>
        )}
        {messages.map((msg) => (
          <div key={msg.id} className={`flex items-end gap-2 ${msg.sender === MessageSender.USER ? 'justify-end' : 'justify-start'}`}>
            {msg.sender === MessageSender.AI && <img src={photo} alt="AI" className="w-6 h-6 rounded-full object-cover self-start" />}
            <div className={`max-w-[75%] rounded-2xl px-4 py-2 text-white ${msg.sender === MessageSender.USER ? 'bg-blue-600 rounded-br-none' : 'bg-gray-700 rounded-bl-none'}`}>
              {/* === TEAM MODIFICATION START === */}
              <p className="text-sm break-words whitespace-pre-wrap">
                {renderMessageWithLinks(msg.text)}
              </p>
              {/* Udemyè¬›åº§ã‚«ãƒ¼ãƒ‰è¡¨ç¤º */}
              {msg.udemyCourse && (
                <UdemyCourseCard course={msg.udemyCourse} />
              )}
              {/* === TEAM MODIFICATION END === */}
            </div>
          </div>
        ))}
         {isLoading && messages[messages.length - 1]?.sender === MessageSender.USER && (
            <div className="flex items-end gap-2 justify-start">
              <img src={currentPhoto} alt="AI" className="w-6 h-6 rounded-full object-cover self-start" />
              <div className="bg-gray-700 rounded-2xl rounded-bl-none px-4 py-2">
                <div className="flex items-center space-x-1">
                    <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce [animation-delay:-0.3s]"></span>
                    <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce [animation-delay:-0.15s]"></span>
                    <span className="h-2 w-2 bg-gray-400 rounded-full animate-bounce"></span>
                </div>
              </div>
            </div>
        )}
      </div>

      {/* Input Area */}
      <div className="p-3 border-t border-gray-700 bg-gray-900">
        <form onSubmit={handleSendMessage} className="flex items-center space-x-2">
          <input
            type="text"
            value={userInput}
            onChange={(e) => setUserInput(e.target.value)}
            placeholder={isListening ? "éŸ³å£°ã‚’èªè­˜ä¸­..." : "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã¾ãŸã¯éŸ³å£°ã§è©±ã—ã¦ãã ã•ã„..."}
            className="flex-1 bg-gray-700 rounded-full px-4 py-2 text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-blue-500"
            disabled={isLoading}
            readOnly={false}
          />
          <button type="submit" disabled={isLoading || !userInput.trim()} className="bg-blue-600 rounded-full p-3 text-white disabled:bg-gray-600 disabled:cursor-not-allowed transition-colors">
            <SendIcon />
          </button>
        </form>
         <button onClick={onEndCall} className="w-full text-center text-red-500 text-sm mt-3 hover:text-red-400">é€šè©±ã‚’çµ‚äº†</button>
      </div>
    </div>
  );
};

export default ChatScreen;